{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Toolio: OpenAI-like HTTP server API for structured LLM response generation, and tool-calling","text":"<p>Toolio is an OpenAI-like HTTP server API that supports structured LLM response generation (e.g., conforming to a JSON schema), designed for Apple Silicon (M1/M2/M3/M4 Macs) using the MLX project.</p>"},{"location":"#structured-control-for-large-language-models","title":"Structured Control for Large Language Models","text":"<p>Large Language Models (LLMs) are powerful but complex machines that generate text based on input prompts. While this simplicity is convenient for casual use, it can be a significant limitation when precise, structured outputs are required. Enter Toolio, an open-source project designed to provide developers with fine-grained control over LLM responses.</p>"},{"location":"#the-challenge-of-llm-control","title":"The Challenge of LLM Control","text":"<p>Imagine a soccer coach who only gives a pep talk and instructions at the start of the game, and then leaves. The team with a coach actively guiding them throughout the match is far more likely to succeed. Similarly, relying solely on initial prompts for LLMs can lead to unpredictable results, especially when generating code, interacting with other applications, or producing specialized content.</p> <p>Toolio acts as that crucial sideline coach for your LLM interactions. It provides a framework for implementing in-process controls, allowing you to guide the LLM's output with precision. This is particularly valuable in scenarios where adherence to specific structures are paramount. To be more specific, Toolio supports structural output enforcement by allowing you to specify JSON schemata which guide the output.</p> <p>As LLMs become increasingly integrated into high-stakes productivity tools, the ability to control and constrain their outputs becomes crucial. Toolio provides developers with the tools they need to harness the power of LLMs while maintaining the precision and reliability required for professional applications.</p> <p>Whether you're building a specialized chatbot, automating content creation, or integrating LLMs into your data analysis pipeline, Toolio offers the flexibility and control you need to deliver consistent, high-quality results.</p>"},{"location":"#empowering-local-llms-with-agentic-capabilities","title":"Empowering Local LLMs with Agentic Capabilities","text":"<p>Structured outputs are the cornerstone of Toolio, and one of the most useful applications of this is in tool-calling, where LLMs can generate requests to tools (functions) specified through the prompt. Toolio allows you to create LLM agentic frameworks, bridging the gap between language models and practical, real-world actions. EVen cooler, Toolio allows you to do this with private, locally-hosted models.</p> <p>Agentic frameworks allow LLMs to interact with external tools and APIs, greatly expanding their capabilities beyond mere text generation. These frameworks enable LLMs to:</p> <ol> <li>Recognize when external tools are needed to complete a task</li> <li>Select the appropriate tool from a predefined set</li> <li>Formulate the correct inputs for the chosen tool</li> <li>Interpret and incorporate the tool's output into their response</li> </ol>"},{"location":"#tool-calling-with-toolio","title":"Tool Calling with Toolio","text":"<p>Toolio's implementation of tool-calling for locally-hosted LLMs opens up a world of possibilities:</p> <ol> <li>Enhanced Decision Making: LLMs can access up-to-date information, perform calculations, or query databases to inform their responses.</li> <li>Task Automation: Complex workflows involving multiple steps and tools can be orchestrated by the LLM.</li> <li>Improved Accuracy: By leveraging specialized tools, LLMs can provide more precise and reliable information.</li> <li>Local Control and Privacy: Unlike cloud-based solutions, Toolio's local approach ensures data privacy and reduces latency.</li> </ol>"},{"location":"#the-synergy-of-structured-outputs-and-tool-calling","title":"The Synergy of Structured Outputs and Tool Calling","text":"<p>Toolio's unique strength lies in the combination of structured outputs and tool-calling capabilities. This synergy creates a powerful ecosystem where:</p> <ol> <li>Reliable Tool Inputs: Structured outputs ensure that tool inputs are consistently formatted and valid.</li> <li>Predictable Workflows: The LLM's actions become more deterministic and easier to audit.</li> <li>Enhanced Error Handling: Structured responses from tools can be easily parsed and acted upon by the LLM.</li> <li>Streamlined Integration: Developers can more easily connect LLMs with existing systems and APIs.</li> </ol> <p>By combining structured outputs with powerful tool-calling capabilities, Toolio sets a new standard for what's possible with locally-hosted LLMs. Whether you're building advanced AI assistants, automating complex workflows, or developing new ways for LLMs to interact with the world, Toolio provides the foundation you need to push the boundaries of AI application development.</p>"},{"location":"#on-the-shoulders-of-giants","title":"On the shoulders of giants","text":"<ul> <li>Toolio began as a fork of the LLM Structured Output project, which is still, through explicit dependency, still key to what makes Toolio tick.</li> </ul>"},{"location":"gettingstarted/","title":"Getting Started","text":"<p>Getting Started with Toolio</p> <p>Toolio is an OpenAI-like HTTP server API implementation that supports structured LLM response generation and reliable tool calling. It's built on the MLX framework for Apple Silicon, making it exclusive to Mac platforms with M1/M2/M3/M4 chips.</p>"},{"location":"gettingstarted/#prerequisites","title":"Prerequisites","text":"<ul> <li>Apple Silicon Mac (M1, M2, M3, or M4)</li> <li>Note: You can install Toolio on other OSes, for example to use the client library or <code>toolio_request</code> to access a Toolio server. In this case, you will not be able use any features which involve loading LLMs.</li> <li>Python 3.10 or more recent (tested only on 3.11 or more recent)</li> </ul> <p>To verify you're on an Apple Silicon Mac you can run:</p> <pre><code>python -c \"import platform; assert 'arm64' in platform.platform()\"\n</code></pre>"},{"location":"gettingstarted/#installation","title":"Installation","text":"<p>Install Toolio using pip:</p> <pre><code>pip install toolio\n</code></pre> <p>For some built-in tools, you'll need additional dependencies:</p> <pre><code>pip install -Ur requirements-extra.txt\n</code></pre>"},{"location":"gettingstarted/#quick-start","title":"Quick Start","text":""},{"location":"gettingstarted/#1-host-a-toolio-server","title":"1. Host a Toolio Server","text":"<p>Launch a Toolio server using an MLX-format LLM:</p> <pre><code>toolio_server --model=mlx-community/Hermes-2-Theta-Llama-3-8B-4bit\n</code></pre> <p>This command downloads and hosts the specified model.</p>"},{"location":"gettingstarted/#2-make-a-basic-request","title":"2. Make a Basic Request","text":"<p>Use the <code>toolio_request</code> command-line tool:</p> <pre><code>toolio_request --apibase=\"http://localhost:8000\" --prompt=\"I am thinking of a number between 1 and 10. Guess what it is.\"\n</code></pre>"},{"location":"gettingstarted/#3-use-structured-output","title":"3. Use Structured Output","text":"<p>Constrain the LLM's output using a JSON schema:</p> <pre><code>export LMPROMPT='Which countries are mentioned in the sentence \"Adamma went home to Nigeria for the hols\"? Your answer should be only JSON, according to this schema: {json_schema}'\nexport LMSCHEMA='{\"type\": \"array\", \"items\": {\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"continent\": {\"type\": \"string\"}}}}'\ntoolio_request --apibase=\"http://localhost:8000\" --prompt=$LMPROMPT --schema=$LMSCHEMA\n</code></pre>"},{"location":"gettingstarted/#4-tool-calling","title":"4. Tool Calling","text":"<p>Use built-in or custom tools:</p> <pre><code>toolio_request --apibase=\"http://localhost:8000\" --tool=toolio.tool.math.calculator --loglevel=DEBUG \\\n--prompt='Usain Bolt ran the 100m race in 9.58s. What was his average velocity?'\n</code></pre>"},{"location":"gettingstarted/#5-python-api-usage","title":"5. Python API Usage","text":"<p>Use Toolio directly in Python:</p> <pre><code>import asyncio\nfrom toolio.llm_helper import model_manager, extract_content\n\ntoolio_mm = model_manager('mlx-community/Hermes-2-Theta-Llama-3-8B-4bit')\n\nasync def say_hello(tmm):\n    msgs = [{\"role\": \"user\", \"content\": \"Hello! How are you?\"}]\n    async for chunk in extract_content(tmm.complete(msgs)):\n        print(chunk, end='')\n\nasyncio.run(say_hello(toolio_mm))\n</code></pre>"},{"location":"gettingstarted/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the <code>demo</code> directory for more examples</li> <li>Explore creating custom tools</li> <li>Learn about LLM-specific flows and flags</li> </ul>"},{"location":"pragmatics/","title":"Practical notes","text":"<p>Practical notes</p>"},{"location":"pragmatics/#managing-the-models-cache","title":"Managing the models cache","text":""},{"location":"pragmatics/#offline-operation","title":"Offline operation","text":"<p>Many users of Toolio will be referencing HuggingFace paths to models, especially in the HF MLX community. By defsult whenever you reference a model in this way it is downloaded and cached on your drive, so you needn't download it again in future, unless it changes upstream (theer's a new checkpoint).</p> <p>This also means you should be able to use Toolio with already downloaded models when you are not connected to the Internet, but some details about how models are loaded add a wrinkle to matters. If you reference any model by its HF path, even if it has already been cached, there will be internet access, and things will hang or fail if you are offline.</p>"},{"location":"pragmatics/#environment-variables","title":"Environment variables","text":"<p>Try the environment variable <code>HF_DATASETS_OFFLINE=\"1\"</code>. Also try setting <code>HF_HOME</code> to a cache folder with predownloaded the necessary models &amp; datasets.</p>"},{"location":"pragmatics/#specifying-the-cache-local-directory","title":"Specifying the cache local directory","text":"<p>One solution ot this is to explicitly load the local cache directory rather than the HF path. An easy first step is to scan the cache to see what's there:</p> <pre><code>mlx_lm.manage --scan --pattern \"\"\n</code></pre> <p>This scan will reveal the HF path as well as local cache dirs for all cached models. Here is one example line from my case:</p> <pre><code>mlx-community/Hermes-2-Theta-Llama-3-8B-4bit model             4.5G        6 6 weeks ago   6 weeks ago   /Users/username/.cache/huggingface/hub/models--mlx-community--Hermes-2-Theta-Llama-3-8B-4bit \n</code></pre> <p>Focusing on the first and last columns, this says that <code>mlx-community/Hermes-2-Theta-Llama-3-8B-4bit</code> is cached at <code>/Users/username/.cache/huggingface/hub/models--mlx-community--Hermes-2-Theta-Llama-3-8B-4bit</code>. If you check that directory, you'll find a <code>snapshots</code>, and one or more hash-named directories under that. You can specify one of these instead of the HF path, e.g.</p> <pre><code>toolio_server --model=\"/Users/username/.cache/huggingface/hub/models--mlx-community--Hermes-2-Theta-Llama-3-8B-4bit/snapshots/a1b2c3d4e5f6g7h8etc/\"\n</code></pre> <p>Loading in this way will avoid any attempts to connect to the internet.</p>"},{"location":"pragmatics/#saving-a-pretrained-model","title":"Saving a pretrained model","text":"<p>If you'd ratehr have a location of your choice, avoid the hash-based names, etc., you can use\u2026</p>"},{"location":"pragmatics/#further-exploration","title":"Further exploration","text":"<p><code>local_files_only</code> argument to <code>snapshot_download</code> (used by mlx_lm under the hood)\u2014(bool, optional, defaults to False) \u2014 If True, avoid downloading the file and return the path to the local cached file if it exists.</p>"}]}