# SPDX-FileCopyrightText: 2024-present Oori Data <info@oori.dev>
#
# SPDX-License-Identifier: Apache-2.0
# toolio.cli.server
'''
LLM server with OpenAI-like API, for structured prompting support & including function/tool calling

Based on: https://github.com/otriscon/llm-structured-output/blob/main/src/examples/server.py

TODO: Break out CLI stuff (new module for FastAPI bits)

toolio_server --model=mlx-community/Hermes-2-Theta-Llama-3-8B-4bit

Note: you can also point `--model` at a downloaded or converted MLX model on local storage.

Set up to use uvicorn. See [this note](https://github.com/fastapi/fastapi/discussions/7299#discussioncomment-5135746)
by the FastAPI maintainer. You can tweak your setitng to use e.g. Gunicorn, Hypercorn or Daphne.

> By default, if you install with pip install "uvicorn[standard]" that will include and use Uvloop,
> a drop-in replacement for the asyncio loop, with very high performance.
> You don't have to do anything in your code to use it, just install uvicorn[standard].
'''

import os
import time
from contextlib import asynccontextmanager
import logging

from fastapi import FastAPI, Request, status
from fastapi.responses import FileResponse, JSONResponse  # , StreamingResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
import click
import uvicorn

from toolio.vendor.llm_structured_output.util.output import info, warning, debug

from toolio.http_schematics import V1ChatCompletionsRequest
# from toolio.common import DEFAULT_FLAGS, FLAGS_LOOKUP
# from toolio.schema_helper import Model
from toolio.llm_helper import local_model_runner
from toolio.http_impl import post_v1_chat_completions_impl


# List of known loggers with too much chatter at debug level
TAME_LOGGERS = ['asyncio', 'httpcore', 'httpx']
for tl in TAME_LOGGERS:
    logging.getLogger(tl).setLevel(logging.WARNING)

# Note: above explicit list is a better idea than some sort of blanket approach such as the following:
# for handler in logging.root.handlers:
#     handler.addFilter(logging.Filter('toolio'))

# There is further log config below, in main()

NUM_CPUS = int(os.cpu_count())
app_params = {}

# Context manager for the FastAPI app's lifespan: https://fastapi.tiangolo.com/advanced/events/
# Don't just stick this in globals, because we're planning for better async support
@asynccontextmanager
async def lifespan(app: FastAPI):
    '''
    Load model one-time
    '''
    # Startup code here. Particularly persistence, so that its DB connection pool is set up in the right event loop
    info(f'Loading model ({app_params["model"]})â€¦')
    tstart = time.perf_counter_ns()
    # Can use click's env support if we decide we want this
    # model_path = os.environ['MODEL_PATH']
    app.state.model_runner = local_model_runner(app_params['model'], server_mode=True)
    app.state.params = app_params
    tdone = time.perf_counter_ns()
    # XXX: alternate ID option is app.state.model.model.model_type which is a string, e.g. 'gemma2'
    # print(app.state.model.model.__class__, app.state.model.model.model_type)
    info(f'Model loaded in {(tdone - tstart)/1000000000.0:.3f}s. Type: {app.state.model_runner.model_type}')
    # Look into exposing control over methods & headers as well
    yield
    # Shutdown code here, if any


app = FastAPI(lifespan=lifespan)


@app.exception_handler(RequestValidationError)
# pylint: disable-next=unused-argument
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    exc_str = f'{exc}'
    warning(f'RequestValidationError: {exc_str}. Request: {await request.json()!r}')
    content = {'status_code': 10422, 'message': exc_str, 'data': None}
    return JSONResponse(
        content=content, status_code=status.HTTP_422_UNPROCESSABLE_ENTITY
    )


@app.get('/status')
def get_status():
    return {'status': 'OK'}


@app.get('/')
def get_root():
    return FileResponse(f'{os.path.dirname(os.path.realpath(__file__))}/static/ui.html')


@app.post('/v1/chat/completions')
async def post_v1_chat_completions(req_data: V1ChatCompletionsRequest):
    # Don't allow streaming requests until we can better define their semantics
    if req_data.stream:
        return JSONResponse(
            content={'status_code': 10400, 'message': 'Streaming requests not supported'},
            status_code=status.HTTP_400_BAD_REQUEST
        )
    debug('REQUEST:', req_data)
    # if req_data.stream:
    #     return StreamingResponse(
    #         content=post_v1_chat_completions_impl(app.state, req_data),
    #         media_type='text/event-stream',
    #     )
    # else:
    response = await post_v1_chat_completions_impl(app.state, req_data)
    debug('RESPONSE:', response.body)
    return response


@click.command()
@click.option('--host', default='127.0.0.1', help='Host nodename/address for the launched server')
@click.option('--port', default=8000, help='Network port for the launched server')
# @click.option('--reload', is_flag=True, help='Reload on code update')
@click.option('--model', type=str, help='HuggingFace ID or disk path for locally-hosted MLF format model')
@click.option('--default-schema',
    help='JSON schema to be used if not provided via API call. Interpolated into {jsonschema} placeholder in prompts')
@click.option('--default-schema-file',
    help='Path to JSON schema to be used if not provided via API call.'
         'Interpolated into {jsonschema} placeholder in prompts')
@click.option('--llmtemp', default='0.1', type=float, help='LLM sampling temperature')
@click.option('--workers', type=int, default=0,
              help='Number of workers processes to spawn (each utilizes one CPU core).'
              'Defaults to $WEB_CONCURRENCY environment variable if available, or 1')
@click.option('--cors_origin', multiple=True,
              help='Origin to be permitted for CORS https://fastapi.tiangolo.com/tutorial/cors/')
@click.option('--loglevel', default='INFO', help='Log level, e.g. DEBUG or INFO')
def main(host, port, model, default_schema, default_schema_file, llmtemp, workers, cors_origin, loglevel):
    global logger  # Remove this line when we modularize
    logging.basicConfig(level=loglevel)
    logger = logging.getLogger(__name__)
    logger.setLevel(loglevel)  # Seems redundant, but is necessary. Python logging is quirky
    app.state.model_runner = logger

    app_params.update(model=model, default_schema=default_schema, default_schema_fpath=default_schema_file,
                      llmtemp=llmtemp)
    app.add_middleware(CORSMiddleware, allow_origins=list(cors_origin), allow_credentials=True,
                       allow_methods=["*"], allow_headers=["*"])
    workers = workers or None
    # logger.info(f'Host has {NUM_CPUS} CPU cores')
    uvicorn.run('toolio.cli.server:app', host=host, port=port, reload=False, workers=workers)


# TODO: Implement log config, probably as a server cli arg
def UNUSED_log_setup(config):
    # Set up logging
    import logging
    global logger  # noqa: PLW0603

    main_loglevel = config.get('log', {'level': 'INFO'})['level']
    logging.config.dictConfig(config['log'])
    # Following 2 lines configure the root logger, so all other loggers in this process space will inherit
    # logging.basicConfig(level=main_loglevel, format='%(levelname)s:%(name)s: %(message)s')
    logging.getLogger().setLevel(main_loglevel)  # Seems redundant, but is necessary. Python logging is quirky
    logger = logging.getLogger(__name__)
    # logger.addFilter(LocalFilter())
